# ============================================
# VAD-Based Recorder - Environment Variables
# ============================================
# Copy this file to .env in the backend directory
# cp env.example .env

# ============================================
# Backend Configuration
# ============================================
API_URL=http://localhost:8000
ALLOWED_ORIGINS=http://localhost:5173,http://127.0.0.1:5173
RECORDINGS_DIR=recordings

# ============================================
# Whisper Transcription Configuration
# ============================================
# Model options: tiny, base, small, medium, large
# - tiny: Fastest, least accurate (good for 300ms response)
# - base: Good balance (recommended)
# - small: Better accuracy
# - medium: High accuracy
# - large: Best accuracy, slowest
WHISPER_MODEL=tiny

# ============================================
# FFmpeg Configuration
# ============================================
# FFmpeg is required for Whisper audio processing
# Ensure FFmpeg is installed and in your system PATH
# 
# Installation:
# - Windows: Download from https://www.gyan.dev/ffmpeg/builds/
#   Add to PATH: C:\ffmpeg\bin
# - Linux: sudo apt-get install ffmpeg
# - macOS: brew install ffmpeg
#
# Verify: ffmpeg -version

# ============================================
# Chatbot Service Configuration
# ============================================
# Primary backend: llama, openai, or huggingface
# The system will try backends in order: primary -> fallbacks
CHATBOT_BACKEND=huggingface

# ============================================
# llama.cpp Configuration (Primary Backend)
# ============================================
# Path to your GGUF model file (relative to backend/ or absolute)
# Download models from: https://huggingface.co/TheBloke
# 
# Recommended models:
# - llama-2-7b-chat.gguf (good balance, ~4GB)
# - llama-2-13b-chat.gguf (better quality, ~7GB, slower)
# - phi-2.Q4_K_M.gguf (smaller, faster, ~1.6GB)
# - mistral-7b-instruct-v0.1.Q4_K_M.gguf (good quality, ~4GB)
#
# Example paths:
# - ./models/llama-2-7b-chat.gguf (relative to backend/)
# - C:/models/llama-2-7b-chat.gguf (absolute Windows)
# - /home/user/models/llama-2-7b-chat.gguf (absolute Linux/Mac)
LLAMA_MODEL_PATH=./models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf

# Context window size (number of tokens)
# Larger = more context, but slower and uses more memory
# Recommended: 2048-4096
LLAMA_N_CTX=2048

# Number of CPU threads for inference
# Set to number of CPU cores for best performance
# Recommended: 4-8
LLAMA_N_THREADS=4

# ============================================
# OpenAI Configuration (Fallback Backend)
# ============================================
# Get your API key from: https://platform.openai.com/api-keys
# Leave empty if not using OpenAI
OPENAI_API_KEY=your_openai_api_key_here

# Model options:
# - gpt-3.5-turbo: Fast, cost-effective (recommended)
# - gpt-4: Higher quality, more expensive
# - gpt-4-turbo-preview: Latest GPT-4 variant
OPENAI_MODEL=gpt-3.5-turbo

# ============================================
# Hugging Face Configuration (Fallback Backend)
# ============================================
# Requirements:
# - Python 3.9+ (already required for backend)
# - transformers library (included in requirements.txt)
# - torch library (included in requirements.txt)
# - Minimum 4GB RAM for small models, 8GB+ recommended
# - Models download automatically on first use (~350MB-1.5GB)
# - For GPU: NVIDIA GPU with CUDA 11.8+ and compatible drivers
#
# Model options:
# - microsoft/DialoGPT-medium: Conversational (recommended, ~350MB, needs 2GB RAM)
# - microsoft/DialoGPT-large: Better quality, larger (~800MB, needs 4GB RAM)
# - gpt2: Smaller, faster (~500MB, needs 2GB RAM)
# - facebook/blenderbot-400M-distill: Conversational (~1.5GB, needs 4GB RAM)
# - microsoft/phi-2: Small, fast (~2GB, needs 4GB RAM)
HF_MODEL_NAME=microsoft/DialoGPT-medium

# Device: cpu or cuda
# - cpu: Works on all systems (slower, but no GPU needed)
#   * Recommended for: Small models (<1GB), testing, development
#   * RAM: 4-8GB minimum
# - cuda: Requires NVIDIA GPU with CUDA (faster) âœ… RECOMMENDED FOR RTX 3050
#   * Recommended for: Production, larger models, faster inference
#   * Requirements:
#     * NVIDIA GPU with CUDA support
#     * CUDA 11.8, 12.1, or 12.4+ (12.4 recommended)
#     * cuDNN library
#     * GPU drivers compatible with CUDA version
#       - CUDA 12.4: Driver 525.60.13+ (Linux) or 528.33+ (Windows)
#     * VRAM: 2GB+ for small models, 4GB+ for medium models
#   * Setup: See GPU_SETUP.md for installation instructions
HF_DEVICE=cuda

# ============================================
# Content Guardrails Configuration
# ============================================
# Path to file containing bad words (one per line, case-insensitive)
# If not provided, uses default minimal list
# 
# To use custom list:
# 1. Copy bad_words.txt.example to bad_words.txt
# 2. Add your words (one per line)
# 3. Set path below (relative to backend/ or absolute)
BAD_WORDS_FILE=./bad_words.txt

# ============================================
# Frontend Configuration (Vite)
# ============================================
# These are used by the frontend build process
# Create a .env file in the frontend/ directory for these

# API URL for frontend to connect to backend
# VITE_API_URL=http://localhost:8000

# Application name
# VITE_APP_NAME=VAD WebRTC Recorder

# Environment: development or production
# VITE_ENVIRONMENT=development

# ============================================
# Setup Instructions
# ============================================
# 1. Copy this file to .env:
#    cp env.example .env
#
# 2. For llama.cpp (recommended for local use):
#    - Visit https://huggingface.co/TheBloke
#    - Download a GGUF model (e.g., llama-2-7b-chat.gguf)
#    - Create models/ directory in backend/
#    - Place model file there
#    - Update LLAMA_MODEL_PATH above
#
# 3. For OpenAI (optional fallback):
#    - Sign up at https://platform.openai.com
#    - Create API key
#    - Add to OPENAI_API_KEY above
#
# 4. For Hugging Face (optional fallback):
#    - Models download automatically on first use
#    - Use CPU for most systems
#    - Use CUDA if you have NVIDIA GPU
#
# 5. For Guardrails:
#    - Copy bad_words.txt.example to bad_words.txt
#    - Add inappropriate words (one per line)
#    - Update BAD_WORDS_FILE path if needed
#
# 6. Frontend .env (create in frontend/ directory):
#    - Copy VITE_* variables to frontend/.env
#    - Or use default values (they have fallbacks)

